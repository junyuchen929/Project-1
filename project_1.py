# -*- coding: utf-8 -*-
"""Project_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12nQAVEtqePXMV96uuaDfEEWW3yE6nQG-
"""

from google.colab import files

uploaded = files.upload()

import pandas as pd
import numpy as np

df = pd.read_csv('dc_md_va_flash_floods_1996_present.csv')

df.head()

df.columns

df['CZ_NAME'].unique()

df['date'] = df['BEGIN_YEARMONTH'] * 100 + df['BEGIN_DAY']
#df['CZ_NAME'] = df['CZ_NAME'].str.replace(r'\s*\(C\)', '', regex=True).str.strip()

#Investigating the Fairfax and Fairfax (C) issue
#unique_combos = df[['date', 'CZ_NAME']].drop_duplicates()
#print(unique_combos.loc[unique_combos['CZ_NAME'] == 'FAIRFAX'])
#print(unique_combos.loc[unique_combos['CZ_NAME'] == 'FAIRFAX (C)'])

#print(df['CZ_NAME'].unique())

df['n_episodes'] = df.groupby(['CZ_NAME', 'date'])['EPISODE_ID'].transform('nunique')
df['n_events'] = df.groupby(['CZ_NAME', 'date'])['EVENT_ID'].transform('nunique')

#summary_df = df.groupby(['CZ_NAME', 'date']).agg(
#    n_events=('EVENT_ID', 'nunique')
#).reset_index()
#
#top_10 = summary_df.sort_values(by='n_events', ascending=False).head(10)

#print(top_10)

def parse_damage(val):
    if pd.isna(val) or val == '':
        return np.nan
    val = str(val).strip().upper().replace('$', '')
    if val.endswith('K'):
        return float(val[:-1]) * 1000
    elif val.endswith('M'):
        return float(val[:-1]) * 1000000
    try:
        return float(val)
    except:
        return np.nan

df['damage_property'] = df['DAMAGE_PROPERTY'].apply(parse_damage)

df['injuries'] = df[['INJURIES_DIRECT', 'INJURIES_INDIRECT']].sum(axis=1, skipna=True)
df['fatalities'] = df[['DEATHS_DIRECT', 'DEATHS_INDIRECT']].sum(axis=1, skipna=True)

df['label'] = None

df.loc[
    (df['injuries'] > 0) |
    (df['fatalities'] > 0) |
    (df['damage_property'] > 0),
    'label'
] = 1

df.loc[
    (df['injuries'] == 0) &
    (df['fatalities'] == 0) &
    (df['damage_property'] == 0),
    'label'
] = 0

exclude_mask = (
    df[['injuries', 'fatalities', 'damage_property']].isna().any(axis=1) &
    (df['injuries'].fillna(0) == 0) &
    (df['fatalities'].fillna(0) == 0) &
    (df['damage_property'].fillna(0) == 0)
)

df = df[~exclude_mask]

df.columns

#new = df[['END_YEARMONTH',
#       'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE', 'STATE_FIPS',
#      'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME',
#       'BEGIN_DATE_TIME', 'CZ_TIMEZONE', 'DAMAGE_PROPERTY','FLOOD_CAUSE', 'CATEGORY', 'BEGIN_RANGE',
#       'date','n_episodes', 'n_events', 'damage_property', 'injuries', 'fatalities',
#       'label']]
#new.shape

df = df.copy()
df['month'] = (df['date'] % 10000) // 100

def get_season(month):
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    elif month in [9, 10, 11]:
        return 'Fall'

df['season'] = df['month'].apply(get_season)

def int_to_datetime(x):
    s = str(int(x))
    return pd.Timestamp(year=int(s[:4]), month=int(s[4:6]), day=int(s[6:8]))

df['date_dt'] = df['date'].apply(int_to_datetime)

df = df.sort_values(['CZ_NAME', 'date_dt'])

df_damage = df[df['label'] == 1].copy()

df_damage['prev_damage_date'] = df_damage.groupby('CZ_NAME')['date_dt'].shift()

df = df.merge(
    df_damage[['CZ_NAME', 'date_dt', 'prev_damage_date']],
    how='left',
    on=['CZ_NAME', 'date_dt']
)

df['prev_damage_date_filled'] = df.groupby('CZ_NAME')['prev_damage_date'].ffill()

df['days_since_last_flood'] = (df['date_dt'] - df['prev_damage_date_filled']).dt.days

df['days_since_last_flood'] = df['days_since_last_flood'].fillna(-1).astype(int)

grouped = df.groupby(['CZ_NAME', 'date'])

county = 'FAIRFAX'
date = 20190708

group = grouped.get_group((county, date))
print(group)

num_groups = df.groupby(['CZ_NAME', 'date']).ngroups
print(num_groups)

episode_counts = df.groupby(['CZ_NAME', 'date'])['EPISODE_ID'].nunique().reset_index()

multiple_episodes = episode_counts[episode_counts['EPISODE_ID'] > 1]

print(multiple_episodes)

grouped = df.groupby(['CZ_NAME', 'date'])

county = 'BALTIMORE'
date = 19960619

group = grouped.get_group((county, date))
print(group)

df['n_episodes']
episode_counts = df.groupby(['CZ_NAME', 'date'])['EPISODE_ID'].nunique()

count_distribution = episode_counts.value_counts().sort_index()

print(count_distribution)

df['CZ_NAME'].unique()

columns_to_keep = [
    'CZ_NAME', 'n_episodes', 'n_events', 'damage_property',
    'injuries', 'fatalities', 'label', 'month', 'season',
    'date_dt', 'prev_damage_date'
]

df_copy = df[columns_to_keep].copy()

df_copy.head()

grouped_df = df.groupby(['CZ_NAME', 'date_dt'], as_index=False).agg({
    'injuries': 'sum',
    'fatalities': 'sum',
    'damage_property': 'sum',
    'n_episodes': 'first',
    'n_events': 'first',
    'label': 'first',
    'month': 'first',
    'season': 'first',
    'prev_damage_date': 'first'
})

grouped_df